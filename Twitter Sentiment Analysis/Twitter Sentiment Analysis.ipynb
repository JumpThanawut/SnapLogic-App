{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.16.1\n",
    "# !pip install requests\n",
    "# !pip install pandas\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset.\n",
    "dataset_url = \"https://github.com/JumpThanawut/dataset/blob/master/twitter_sentiment_analysis/tweet_1600000.csv.zip?raw=true\"\n",
    "urllib.request.urlretrieve(dataset_url, \"tweet_1600000.csv.zip\")  \n",
    "with ZipFile(\"tweet_1600000.csv.zip\", \"r\") as zipObj:\n",
    "    zipObj.extractall()\n",
    "df = pd.read_csv(\"tweet_1600000.csv\", header=None, encoding='latin-1')\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sentiment and text columns.\n",
    "sentiment_list = [0 if x == 0 else 1 for x in df[0]]\n",
    "tweet_list = df[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tokenizer on tweets.\n",
    "num_words = 200\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, lower=True)\n",
    "tokenizer.fit_on_texts(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 690960\n",
      "Sample of Padded Tokenized Tweet:\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  15  90  11   3  81 148  94 172  39  58]\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenizer on tweets.\n",
    "tweet_frame = tokenizer.texts_to_sequences(tweet_list)\n",
    "padded_tweet_frame = tf.keras.preprocessing.sequence.pad_sequences(tweet_frame, maxlen=30)\n",
    "num_words = len(tokenizer.word_counts)\n",
    "print(\"Word: {:d}\".format(num_words))\n",
    "print(\"Sample of Padded Tokenized Tweet:\")\n",
    "print(padded_tweet_frame[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets: 1600000\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train and test.\n",
    "num_tweets = len(sentiment_list)\n",
    "print(\"Tweets: {:d}\".format(num_tweets))\n",
    "split_index = int(num_tweets * 0.8)\n",
    "x_train = padded_tweet_frame[0:split_index]\n",
    "x_test = padded_tweet_frame[split_index:]\n",
    "y_train = sentiment_list[0:split_index]\n",
    "y_test = sentiment_list[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 1280000 samples, validate on 320000 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "1280000/1280000 [==============================] - 38s 30us/sample - loss: 0.5717 - acc: 0.6977 - val_loss: 0.5529 - val_acc: 0.7105\n",
      "Epoch 2/10\n",
      "1280000/1280000 [==============================] - 37s 29us/sample - loss: 0.5495 - acc: 0.7142 - val_loss: 0.5432 - val_acc: 0.7177\n",
      "Epoch 3/10\n",
      "1280000/1280000 [==============================] - 37s 29us/sample - loss: 0.5438 - acc: 0.7178 - val_loss: 0.5395 - val_acc: 0.7204\n",
      "Epoch 4/10\n",
      "1280000/1280000 [==============================] - 38s 29us/sample - loss: 0.5407 - acc: 0.7205 - val_loss: 0.5364 - val_acc: 0.7224\n",
      "Epoch 5/10\n",
      "1280000/1280000 [==============================] - 38s 30us/sample - loss: 0.5379 - acc: 0.7226 - val_loss: 0.5345 - val_acc: 0.7235\n",
      "Epoch 6/10\n",
      "1280000/1280000 [==============================] - 38s 29us/sample - loss: 0.5357 - acc: 0.7242 - val_loss: 0.5326 - val_acc: 0.7252\n",
      "Epoch 7/10\n",
      "1280000/1280000 [==============================] - 38s 30us/sample - loss: 0.5338 - acc: 0.7255 - val_loss: 0.5303 - val_acc: 0.7269\n",
      "Epoch 8/10\n",
      "1280000/1280000 [==============================] - 38s 29us/sample - loss: 0.5317 - acc: 0.7272 - val_loss: 0.5287 - val_acc: 0.7282\n",
      "Epoch 9/10\n",
      "1280000/1280000 [==============================] - 38s 29us/sample - loss: 0.5301 - acc: 0.7285 - val_loss: 0.5266 - val_acc: 0.7299\n",
      "Epoch 10/10\n",
      "1280000/1280000 [==============================] - 38s 29us/sample - loss: 0.5285 - acc: 0.7296 - val_loss: 0.5255 - val_acc: 0.7304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f40c3cfe5c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(num_words, 32))\n",
    "model.add(tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=2048,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
